{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e47e6b-2a90-4680-b6a8-0924efa50ea5",
   "metadata": {},
   "source": [
    "# Face Re-ID\n",
    "\n",
    "This notebook shows an example of training a face re-ID model (`FaceIdentifier`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf5cc5-5fd3-473b-af3b-480f5b0a3b01",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "The data is from CelebA Dataset. For this Face re-ID task, only annotations of identities (`identity_CelebA.txt`) are needed. Each line of the annotations is `image_path identity` (e.g. `000001.jpg 2880`). I splited the data into 3 parts: training_set, val_set, and test_set after grouping them by identities. In every epoch, the model will be trained on training_set, and be monitored by computing validation loss on val_set.\n",
    "\n",
    "I customed a class `IdentityDataset`, returning 3 images in tensor format, the first 2 images are from the same person, and the third is randomly chose from another person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8177da9-bfa4-4b70-9662-55eadca377c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e625d-7533-4d77-b7ef-825cf3db7563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from face_identifier.datasets import read_identities, IdentityDataset, split_train_valid_test\n",
    "\n",
    "\n",
    "# paths\n",
    "path = '/home/anthony/Documents/Homework Documents/Inno Lab Group Project/'\n",
    "path_anno = path + 'identity_CelebA.txt'\n",
    "# path_images = path + 'cropped'\n",
    "path_images = path + 'img_align_celeba'\n",
    "\n",
    "# read data\n",
    "data = read_identities(path_anno, path_images, n_images=2)\n",
    "training_index, validation_index, test_index = split_train_valid_test(data)\n",
    "training_set = IdentityDataset(data, path_images, training_index, transform=None)\n",
    "val_set = IdentityDataset(data, path_images, validation_index, transform=None)\n",
    "test_set = IdentityDataset(data, path_images, test_index, transform=None)\n",
    "\n",
    "# show images\n",
    "training_set.draw(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8825b222-3752-4635-912c-9a1bdeff6a91",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "\n",
    "CNN model is suitable in this feature extraction task. The core model `FaceIdentifier`, returning face ID as a 500 dims vector, is based on a powerful CNN ResNet50, with a extra full connecting network that includes a ReLU and a linear layer. A triplet model, wrapping the core model, is also implemented for training. It receives three images mentioned above as inputs, and returns 2 scalars representing the probabilities that image 2 and 3 are from the same person of image 1. The expecting label is `[1, 0]` because of the design of the dataset. \n",
    "\n",
    "I chose Adam optimizer and MSE loss function to train this model, because I want the prediction of the triplet model as close as `[1, 0]`.\n",
    "\n",
    "During the training, the core model's parameters will be updated. Finally, all we need for generating face ID is the core model. The architecture is shown below:\n",
    "\n",
    "![architecture](../docs/pictures/face-identifier.drawio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34d244d-ec7f-4c76-b2bd-95fa18868024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_identifier.models import FaceIdentifier, TripletModel\n",
    "\n",
    "\n",
    "model = FaceIdentifier()\n",
    "tri_model = TripletModel(model)\n",
    "optimizer = torch.optim.Adam(tri_model.parameters(), lr=1e-3)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "tri_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bdf18-6715-435e-8d76-bef7a1e11f7d",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "During the training, the `ModelTrainer` will record the training and validation losses in every epoch. It will also save the state dict of the model with the lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef85224-1f13-4130-a660-83ffa77b5a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from face_identifier.processes import ModelTrainer, test\n",
    "\n",
    "\n",
    "save_path = 'face-identifier-wrapped.pt'\n",
    "if os.path.exists(save_path):\n",
    "    tri_model.load_state_dict(torch.load(save_path))\n",
    "trainer = ModelTrainer(tri_model, training_set, val_set, optimizer, loss_func, device=device, batch_size=30,\n",
    "                       save_path=save_path, log_name='Id Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61163647-8207-4b92-8bff-074adc70c43a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ff75dc-7c42-49b3-ba3a-e633b43d8d0f",
   "metadata": {},
   "source": [
    "According to the learning curve, overfitting occurred after the 68th epoch.\n",
    "![learning curve](../docs/pictures/learning-curve.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965852c-0544-4b01-84b6-c7623bbc2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "tri_model.load_state_dict(torch.load('face-identifier-wrapped.pt'))\n",
    "\n",
    "# save the core model\n",
    "torch.save(model.state_dict(), 'face-identifier.pt')\n",
    "\n",
    "print(f'Best validation loss is {trainer.best_loss: .5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c187ec-21e8-47cf-a3fd-78537f05c61f",
   "metadata": {},
   "source": [
    "## Evaluation & Visualization\n",
    "\n",
    "First, we need to check the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950758c-5e28-4a5b-8288-ef8f9cf456eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "from face_identifier.models import FaceIdentifier, TripletModel\n",
    "model = FaceIdentifier.load('face-identifier.pt')\n",
    "tri_model = TripletModel(model)\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c03c962-aa06-4bda-9dbe-b1028bb53e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from face_identifier.processes import test, MODEL_TO_FUNC\n",
    "\n",
    "\n",
    "test_loss = test(tri_model, tqdm(DataLoader(test_set, batch_size=30), 'Testing'), MODEL_TO_FUNC['identifier'], loss_func, device)\n",
    "print(f'Test loss is {test_loss: .5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6bd83a-2477-436d-b4fa-874e4ef1355a",
   "metadata": {},
   "source": [
    "The test loss roughly equals to the best validation loss, here we got the best model!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33909f-bb99-4c75-9e35-092b37bd64c0",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The core model can be evaluated by being wrapped as a binary classification model. The new model will predict whether 2 given images are from the same person. A special dataset, returning 2 images and a binary label, is also needed. The function `evaluate_classification` can handle these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9a658-2dc2-4909-bdf4-e10fcc23e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_identifier.evaluations import evaluate_classification\n",
    "\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "\n",
    "model = FaceIdentifier.load('face-identifier.pt')\n",
    "result = evaluate_classification(model, test_set, n_samples=100,\n",
    "                                 threshold=threshold,\n",
    "                                 batch_size=64, device=device)\n",
    "\n",
    "print(f'Tested on {result[\"total\"]} pairs of images.')\n",
    "for key in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "    print(f'{key.title()}: {result[key]:.5%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa437557-3024-4dbc-9aef-e600175df5c8",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "You can also check the similarities of specific pictures. In this case, **similarity > 0.8** means two images are from the same person. The function `visualize` will use the vectors returned by the core model to compute the similarities of the given three images, and show the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3224da-8959-4ca0-bf6f-3287adb2b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_identifier.evaluations import visualize\n",
    "\n",
    "for i in range(5):\n",
    "    visualize(model, test_set, i + 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e95efc-bac4-4872-9ee3-34d2c3e38b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
